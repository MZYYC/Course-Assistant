#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„æ–‡æœ¬åˆ†å‰²å™¨
æ”¯æŒçˆ¶å­åˆ‡å—ç­–ç•¥ã€æŒä¹…åŒ–é›†æˆå’Œæ€§èƒ½ç›‘æ§
"""

import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import uuid
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import Config


class ParentChildTextSplitter:
    """çˆ¶å­åˆ‡å—æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        parent_chunk_size: int = 2000,
        parent_chunk_overlap: int = 200,
        child_chunk_size: int = 400,
        child_chunk_overlap: int = 50,
        separators: Optional[List[str]] = None,
        persistence_manager=None
    ):
        """
        åˆå§‹åŒ–çˆ¶å­æ–‡æœ¬åˆ†å‰²å™¨
        
        Args:
            parent_chunk_size: çˆ¶å—å¤§å°
            parent_chunk_overlap: çˆ¶å—é‡å 
            child_chunk_size: å­å—å¤§å°
            child_chunk_overlap: å­å—é‡å 
            separators: åˆ†å‰²ç¬¦åˆ—è¡¨
            persistence_manager: æŒä¹…åŒ–ç®¡ç†å™¨
        """
        self.parent_chunk_size = parent_chunk_size
        self.parent_chunk_overlap = parent_chunk_overlap
        self.child_chunk_size = child_chunk_size
        self.child_chunk_overlap = child_chunk_overlap
        self.persistence_manager = persistence_manager
        
        # åˆ†å‰²ç»Ÿè®¡
        self.split_stats = {
            'total_documents': 0,
            'total_parent_chunks': 0,
            'total_child_chunks': 0,
            'processing_time': 0,
            'last_split_time': None
        }
        
        # é»˜è®¤åˆ†å‰²ç¬¦
        if separators is None:
            separators = [
                "\n\n\n",  # æ®µè½åˆ†å‰²
                "\n\n",    # åŒæ¢è¡Œ
                "\n",      # å•æ¢è¡Œ
                "ã€‚",      # ä¸­æ–‡å¥å·
                "ï¼",      # ä¸­æ–‡æ„Ÿå¹å·
                "ï¼Ÿ",      # ä¸­æ–‡é—®å·
                ".",       # è‹±æ–‡å¥å·
                "!",       # è‹±æ–‡æ„Ÿå¹å·
                "?",       # è‹±æ–‡é—®å·
                ";",       # åˆ†å·
                ",",       # é€—å·
                " ",       # ç©ºæ ¼
                ""         # å­—ç¬¦çº§åˆ«
            ]
        
        # åˆ›å»ºçˆ¶å—åˆ†å‰²å™¨
        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=parent_chunk_size,
            chunk_overlap=parent_chunk_overlap,
            length_function=len,
            separators=separators
        )
        
        # åˆ›å»ºå­å—åˆ†å‰²å™¨
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=child_chunk_size,
            chunk_overlap=child_chunk_overlap,
            length_function=len,
            separators=separators
        )
        
        print(f"çˆ¶å­æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ: çˆ¶å—={parent_chunk_size}, å­å—={child_chunk_size}")
    
    def split_documents(self, documents: List[Document]) -> Dict[str, Any]:
        """
        åˆ†å‰²æ–‡æ¡£ä¸ºçˆ¶å­å—
        
        Args:
            documents: è¾“å…¥æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åŒ…å«parent_docså’Œchild_docsçš„å­—å…¸
        """
        start_time = time.time()
        parent_docs = []
        child_docs = []
        parent_to_children = {}  # çˆ¶å—IDåˆ°å­å—IDåˆ—è¡¨çš„æ˜ å°„
        
        print(f"ğŸ”§ å¼€å§‹çˆ¶å­åˆ‡å—ï¼Œè¾“å…¥æ–‡æ¡£: {len(documents)}")
        
        for doc_idx, doc in enumerate(documents):
            print(f"   å¤„ç†æ–‡æ¡£ [{doc_idx+1}/{len(documents)}]: {doc.metadata.get('source', 'unknown')}")
            
            # åˆ†å‰²ä¸ºçˆ¶å—
            parent_chunks = self.parent_splitter.split_documents([doc])
            
            for parent_idx, parent_chunk in enumerate(parent_chunks):
                # ä¸ºçˆ¶å—ç”Ÿæˆå”¯ä¸€ID
                parent_id = str(uuid.uuid4())
                parent_chunk.metadata.update({
                    "chunk_id": parent_id,
                    "chunk_type": "parent",
                    "parent_index": parent_idx,
                    "original_source": doc.metadata.get("source", "unknown"),
                    "split_timestamp": datetime.now().isoformat(),
                    "chunk_size": len(parent_chunk.page_content)
                })
                parent_docs.append(parent_chunk)
                
                # å°†çˆ¶å—è¿›ä¸€æ­¥åˆ†å‰²ä¸ºå­å—
                child_chunks = self.child_splitter.split_documents([parent_chunk])
                child_ids = []
                
                for i, child_chunk in enumerate(child_chunks):
                    # ä¸ºå­å—ç”Ÿæˆå”¯ä¸€ID
                    child_id = f"{parent_id}_child_{i}"
                    child_chunk.metadata.update({
                        "chunk_id": child_id,
                        "chunk_type": "child",
                        "parent_id": parent_id,
                        "child_index": i,
                        "parent_index": parent_idx,
                        "original_source": doc.metadata.get("source", "unknown"),
                        "split_timestamp": datetime.now().isoformat(),
                        "chunk_size": len(child_chunk.page_content)
                    })
                    child_docs.append(child_chunk)
                    child_ids.append(child_id)
                
                parent_to_children[parent_id] = child_ids
        
        processing_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.split_stats.update({
            'total_documents': len(documents),
            'total_parent_chunks': len(parent_docs),
            'total_child_chunks': len(child_docs),
            'processing_time': processing_time,
            'last_split_time': datetime.now().isoformat()
        })
        
        print(f"âœ… çˆ¶å­åˆ‡å—å®Œæˆ: {len(parent_docs)} ä¸ªçˆ¶å—, {len(child_docs)} ä¸ªå­å—, è€—æ—¶ {processing_time:.2f}s")
        
        # è®°å½•åˆ°æŒä¹…åŒ–ç³»ç»Ÿ
        if self.persistence_manager:
            try:
                self.persistence_manager.save_document_history(
                    file_path="text_splitting",
                    file_name="parent_child_split",
                    file_size=sum(len(doc.page_content) for doc in documents),
                    processing_time=processing_time,
                    success=True,
                    chunk_count=len(parent_docs) + len(child_docs),
                    strategy="parent_child"
                )
            except Exception as e:
                print(f"âš ï¸ æŒä¹…åŒ–è®°å½•å¤±è´¥: {e}")
        
        return {
            "parent_docs": parent_docs,
            "child_docs": child_docs,
            "parent_to_children": parent_to_children,
            "stats": self.split_stats
        }
    
    def get_parent_for_child(self, child_doc: Document) -> Optional[str]:
        """è·å–å­å—å¯¹åº”çš„çˆ¶å—ID"""
        return child_doc.metadata.get("parent_id")
    
    def get_context_for_child(
        self, 
        child_doc: Document, 
        parent_docs: List[Document]
    ) -> Optional[Document]:
        """ä¸ºå­å—è·å–å®Œæ•´çš„çˆ¶å—ä¸Šä¸‹æ–‡"""
        parent_id = self.get_parent_for_child(child_doc)
        if not parent_id:
            return None
        
        for parent_doc in parent_docs:
            if parent_doc.metadata.get("chunk_id") == parent_id:
                return parent_doc
        
        return None
    
    def get_split_statistics(self) -> Dict[str, Any]:
        """è·å–åˆ†å‰²ç»Ÿè®¡ä¿¡æ¯"""
        return self.split_stats.copy()
    
    def analyze_chunk_distribution(self, chunks: List[Document]) -> Dict[str, Any]:
        """åˆ†æå—åˆ†å¸ƒç»Ÿè®¡"""
        if not chunks:
            return {}
        
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        
        return {
            'total_chunks': len(chunks),
            'avg_chunk_size': sum(chunk_sizes) / len(chunk_sizes),
            'min_chunk_size': min(chunk_sizes),
            'max_chunk_size': max(chunk_sizes),
            'total_characters': sum(chunk_sizes),
            'chunk_types': {
                'parent': len([c for c in chunks if c.metadata.get('chunk_type') == 'parent']),
                'child': len([c for c in chunks if c.metadata.get('chunk_type') == 'child'])
            }
        }


class HybridTextSplitter:
    """æ··åˆæ–‡æœ¬åˆ†å‰²å™¨ï¼Œç»“åˆå¤šç§åˆ†å‰²ç­–ç•¥"""
    
    def __init__(self, persistence_manager=None):
        """åˆå§‹åŒ–æ··åˆæ–‡æœ¬åˆ†å‰²å™¨"""
        self.persistence_manager = persistence_manager
        
        # æ ‡å‡†åˆ†å‰²å™¨
        self.standard_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", ";", ",", " ", ""]
        )
        
        # çˆ¶å­åˆ†å‰²å™¨
        self.parent_child_splitter = ParentChildTextSplitter(
            parent_chunk_size=Config.PARENT_CHUNK_SIZE,
            parent_chunk_overlap=Config.PARENT_CHUNK_OVERLAP,
            child_chunk_size=Config.CHILD_CHUNK_SIZE,
            child_chunk_overlap=Config.CHILD_CHUNK_OVERLAP,
            persistence_manager=persistence_manager
        )
        
        # è¯­ä¹‰åˆ†å‰²å™¨ï¼ˆåŸºäºå¥å­ï¼‰
        self.semantic_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP // 2,
            length_function=len,
            separators=["\n\n\n", "\n\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", "\n", " "]
        )
        
        # åˆ†å‰²ç»Ÿè®¡
        self.hybrid_stats = {
            'strategies_used': [],
            'total_processing_time': 0,
            'last_strategy': None,
            'performance_comparison': {}
        }
        
        print(f"æ··åˆæ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒç­–ç•¥: standard, parent_child, semantic, hybrid")
    
    def split_documents_with_strategy(
        self, 
        documents: List[Document], 
        strategy: str = "parent_child"
    ) -> Dict[str, Any]:
        """
        æ ¹æ®ç­–ç•¥åˆ†å‰²æ–‡æ¡£
        
        Args:
            documents: è¾“å…¥æ–‡æ¡£åˆ—è¡¨
            strategy: åˆ†å‰²ç­–ç•¥ ("standard", "parent_child", "semantic", "hybrid")
            
        Returns:
            åˆ†å‰²ç»“æœå­—å…¸
        """
        start_time = time.time()
        print(f"ğŸ”§ ä½¿ç”¨ '{strategy}' ç­–ç•¥åˆ†å‰² {len(documents)} ä¸ªæ–‡æ¡£")
        
        result = {}
        
        if strategy == "standard":
            chunks = self.standard_splitter.split_documents(documents)
            # æ·»åŠ åˆ†å‰²å…ƒæ•°æ®
            for chunk in chunks:
                chunk.metadata.update({
                    "split_strategy": "standard",
                    "split_timestamp": datetime.now().isoformat(),
                    "chunk_size": len(chunk.page_content)
                })
            result = {"chunks": chunks, "strategy": "standard"}
        
        elif strategy == "parent_child":
            result = self.parent_child_splitter.split_documents(documents)
            result["strategy"] = "parent_child"
        
        elif strategy == "semantic":
            chunks = self.semantic_splitter.split_documents(documents)
            # æ·»åŠ åˆ†å‰²å…ƒæ•°æ®
            for chunk in chunks:
                chunk.metadata.update({
                    "split_strategy": "semantic",
                    "split_timestamp": datetime.now().isoformat(),
                    "chunk_size": len(chunk.page_content)
                })
            result = {"chunks": chunks, "strategy": "semantic"}
        
        elif strategy == "hybrid":
            # æ··åˆç­–ç•¥ï¼šåŒæ—¶ä½¿ç”¨å¤šç§åˆ†å‰²æ–¹æ³•
            print("   ğŸ”„ æ‰§è¡Œæ··åˆåˆ†å‰²ç­–ç•¥...")
            
            # æ ‡å‡†åˆ†å‰²
            standard_start = time.time()
            standard_chunks = self.standard_splitter.split_documents(documents)
            standard_time = time.time() - standard_start
            
            # çˆ¶å­åˆ†å‰²
            parent_child_start = time.time()
            parent_child_result = self.parent_child_splitter.split_documents(documents)
            parent_child_time = time.time() - parent_child_start
            
            # è¯­ä¹‰åˆ†å‰²
            semantic_start = time.time()
            semantic_chunks = self.semantic_splitter.split_documents(documents)
            semantic_time = time.time() - semantic_start
            
            # ä¸ºä¸åŒç­–ç•¥çš„å—æ·»åŠ æ ‡è¯†
            for chunk in standard_chunks:
                chunk.metadata.update({
                    "split_strategy": "standard",
                    "split_timestamp": datetime.now().isoformat(),
                    "chunk_size": len(chunk.page_content)
                })
            
            for chunk in parent_child_result["parent_docs"]:
                chunk.metadata["split_strategy"] = "parent"
            
            for chunk in parent_child_result["child_docs"]:
                chunk.metadata["split_strategy"] = "child"
            
            for chunk in semantic_chunks:
                chunk.metadata.update({
                    "split_strategy": "semantic",
                    "split_timestamp": datetime.now().isoformat(),
                    "chunk_size": len(chunk.page_content)
                })
            
            # æ€§èƒ½å¯¹æ¯”
            performance_comparison = {
                "standard": {"time": standard_time, "chunks": len(standard_chunks)},
                "parent_child": {"time": parent_child_time, "chunks": len(parent_child_result["parent_docs"]) + len(parent_child_result["child_docs"])},
                "semantic": {"time": semantic_time, "chunks": len(semantic_chunks)}
            }
            
            result = {
                "standard_chunks": standard_chunks,
                "parent_docs": parent_child_result["parent_docs"],
                "child_docs": parent_child_result["child_docs"],
                "semantic_chunks": semantic_chunks,
                "parent_to_children": parent_child_result["parent_to_children"],
                "strategy": "hybrid",
                "performance_comparison": performance_comparison
            }
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å‰²ç­–ç•¥: {strategy}")
        
        processing_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.hybrid_stats['strategies_used'].append(strategy)
        self.hybrid_stats['total_processing_time'] += processing_time
        self.hybrid_stats['last_strategy'] = strategy
        if strategy == "hybrid" and "performance_comparison" in result:
            self.hybrid_stats['performance_comparison'] = result["performance_comparison"]
        
        # è®°å½•åˆ°æŒä¹…åŒ–ç³»ç»Ÿ
        if self.persistence_manager:
            try:
                total_chunks = 0
                if strategy == "standard" or strategy == "semantic":
                    total_chunks = len(result.get("chunks", []))
                elif strategy == "parent_child":
                    total_chunks = len(result.get("parent_docs", [])) + len(result.get("child_docs", []))
                elif strategy == "hybrid":
                    total_chunks = (len(result.get("standard_chunks", [])) + 
                                  len(result.get("parent_docs", [])) + 
                                  len(result.get("child_docs", [])) + 
                                  len(result.get("semantic_chunks", [])))
                
                self.persistence_manager.save_document_history(
                    file_path="text_splitting_hybrid",
                    file_name=f"split_{strategy}",
                    file_size=sum(len(doc.page_content) for doc in documents),
                    processing_time=processing_time,
                    success=True,
                    chunk_count=total_chunks,
                    strategy=strategy
                )
            except Exception as e:
                print(f"âš ï¸ æŒä¹…åŒ–è®°å½•å¤±è´¥: {e}")
        
        print(f"âœ… '{strategy}' ç­–ç•¥åˆ†å‰²å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f}s")
        
        # æ·»åŠ å¤„ç†æ—¶é—´åˆ°ç»“æœ
        result["processing_time"] = processing_time
        result["hybrid_stats"] = self.hybrid_stats.copy()
        
        return result
    
    def get_hybrid_statistics(self) -> Dict[str, Any]:
        """è·å–æ··åˆåˆ†å‰²å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.hybrid_stats.copy()
    
    def compare_strategies(self, documents: List[Document]) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„æ€§èƒ½"""
        print(f"ğŸ” å¼€å§‹ç­–ç•¥æ€§èƒ½æ¯”è¾ƒ...")
        
        strategies = ["standard", "parent_child", "semantic"]
        comparison_results = {}
        
        for strategy in strategies:
            print(f"   æµ‹è¯•ç­–ç•¥: {strategy}")
            start_time = time.time()
            result = self.split_documents_with_strategy(documents, strategy)
            end_time = time.time()
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_chunks = 0
            total_chars = 0
            
            if strategy == "parent_child":
                chunks = result.get("parent_docs", []) + result.get("child_docs", [])
            else:
                chunks = result.get("chunks", [])
            
            total_chunks = len(chunks)
            total_chars = sum(len(chunk.page_content) for chunk in chunks)
            avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0
            
            comparison_results[strategy] = {
                "processing_time": end_time - start_time,
                "total_chunks": total_chunks,
                "total_characters": total_chars,
                "avg_chunk_size": avg_chunk_size,
                "chunks_per_second": total_chunks / (end_time - start_time) if (end_time - start_time) > 0 else 0
            }
        
        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        fastest_strategy = min(comparison_results.keys(), 
                             key=lambda x: comparison_results[x]["processing_time"])
        most_efficient = min(comparison_results.keys(), 
                           key=lambda x: comparison_results[x]["avg_chunk_size"])
        
        comparison_results["summary"] = {
            "fastest_strategy": fastest_strategy,
            "most_efficient_strategy": most_efficient,
            "total_documents": len(documents),
            "comparison_timestamp": datetime.now().isoformat()
        }
        
        print(f"âœ… ç­–ç•¥æ¯”è¾ƒå®Œæˆï¼Œæœ€å¿«ç­–ç•¥: {fastest_strategy}")
        
        return comparison_results
    
    def recommend_strategy(self, documents: List[Document]) -> str:
        """æ ¹æ®æ–‡æ¡£ç‰¹å¾æ¨èæœ€ä½³åˆ†å‰²ç­–ç•¥"""
        if not documents:
            return "standard"
        
        # åˆ†ææ–‡æ¡£ç‰¹å¾
        total_length = sum(len(doc.page_content) for doc in documents)
        avg_doc_length = total_length / len(documents)
        max_doc_length = max(len(doc.page_content) for doc in documents)
        
        # æ£€æŸ¥æ–‡æ¡£ç»“æ„ç‰¹å¾
        has_structured_content = any(
            any(marker in doc.page_content for marker in ["\n\n", "##", "###", "ã€‚", "ï¼", "ï¼Ÿ"])
            for doc in documents
        )
        
        # æ¨èç­–ç•¥
        if avg_doc_length > 5000 and has_structured_content:
            return "parent_child"
        elif avg_doc_length > 2000 and has_structured_content:
            return "semantic"
        elif max_doc_length > 10000:
            return "hybrid"
        else:
            return "standard"


# è¾…åŠ©å‡½æ•°
def create_text_splitter(strategy: str = "parent_child", persistence_manager=None) -> HybridTextSplitter:
    """åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨çš„å·¥å‚å‡½æ•°"""
    return HybridTextSplitter(persistence_manager=persistence_manager)


def analyze_document_structure(documents: List[Document]) -> Dict[str, Any]:
    """åˆ†ææ–‡æ¡£ç»“æ„ç‰¹å¾"""
    if not documents:
        return {}
    
    total_docs = len(documents)
    total_length = sum(len(doc.page_content) for doc in documents)
    
    # ç»Ÿè®¡å„ç§ç»“æ„ç‰¹å¾
    structure_analysis = {
        "total_documents": total_docs,
        "total_characters": total_length,
        "avg_document_length": total_length / total_docs,
        "min_document_length": min(len(doc.page_content) for doc in documents),
        "max_document_length": max(len(doc.page_content) for doc in documents),
        "has_paragraphs": sum(1 for doc in documents if "\n\n" in doc.page_content),
        "has_chinese_sentences": sum(1 for doc in documents if any(punct in doc.page_content for punct in ["ã€‚", "ï¼", "ï¼Ÿ"])),
        "has_english_sentences": sum(1 for doc in documents if any(punct in doc.page_content for punct in [".", "!", "?"])),
        "has_lists": sum(1 for doc in documents if any(marker in doc.page_content for marker in ["- ", "* ", "1. ", "2. "])),
        "has_headers": sum(1 for doc in documents if any(header in doc.page_content for header in ["#", "##", "###"]))
    }
    
    return structure_analysis
